{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anamod\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'U:\\\\Hieu\\\\CARDIA_longi_project\\\\code\\\\git_code'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('U:\\Hieu\\CARDIA_longi_project\\Dynamic-DeepHit-master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_EPSILON = 1e-08\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import import_data as impt\n",
    "from tf_slim.layers import layers as _layers\n",
    "from class_DeepLongitudinal import Model_Longitudinal_Attention\n",
    "\n",
    "from utils_eval             import c_index, brier_score\n",
    "from utils_log              import save_logging, load_logging\n",
    "from utils_helper           import f_get_minibatch, f_get_boosted_trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _f_get_pred(sess, model, data, data_mi, pred_horizon):\n",
    "    '''\n",
    "        predictions based on the prediction time.\n",
    "        create new_data and new_mask2 that are available previous or equal to the prediction time (no future measurements are used)\n",
    "    '''\n",
    "    new_data    = np.zeros(np.shape(data))\n",
    "    new_data_mi = np.zeros(np.shape(data_mi))\n",
    "\n",
    "    meas_time = np.concatenate([np.zeros([np.shape(data)[0], 1]), np.cumsum(data[:, :, 0], axis=1)[:, :-1]], axis=1)\n",
    "\n",
    "    for i in range(np.shape(data)[0]):\n",
    "        last_meas = np.sum(meas_time[i, :] <= pred_horizon)\n",
    "\n",
    "        new_data[i, :last_meas, :]    = data[i, :last_meas, :]\n",
    "        new_data_mi[i, :last_meas, :] = data_mi[i, :last_meas, :]\n",
    "\n",
    "    return model.predict(new_data, new_data_mi)\n",
    "\n",
    "\n",
    "def f_get_risk_predictions(sess, model, data_, data_mi_, pred_time, eval_time):\n",
    "    \n",
    "    pred = _f_get_pred(sess, model, data_[[0]], data_mi_[[0]], 0)\n",
    "    _, num_Event, num_Category = np.shape(pred)\n",
    "       \n",
    "    risk_all = {}\n",
    "    for k in range(num_Event):\n",
    "        risk_all[k] = np.zeros([np.shape(data_)[0], len(pred_time), len(eval_time)])\n",
    "            \n",
    "    for p, p_time in enumerate(pred_time):\n",
    "        ### PREDICTION\n",
    "        pred_horizon = int(p_time)\n",
    "        pred = _f_get_pred(sess, model, data_, data_mi_, pred_horizon)\n",
    "\n",
    "\n",
    "        for t, t_time in enumerate(eval_time):\n",
    "            eval_horizon = int(t_time) + pred_horizon #if eval_horizon >= num_Category, output the maximum...\n",
    "\n",
    "            # calculate F(t | x, Y, t >= t_M) = \\sum_{t_M <= \\tau < t} P(\\tau | x, Y, \\tau > t_M)\n",
    "            risk = np.sum(pred[:,:,pred_horizon:(eval_horizon+1)], axis=2) #risk score until eval_time\n",
    "            risk = risk / (np.sum(np.sum(pred[:,:,pred_horizon:], axis=2), axis=1, keepdims=True) +_EPSILON) #conditioniong on t > t_pred\n",
    "            \n",
    "            for k in range(num_Event):\n",
    "                risk_all[k][:, p, t] = risk[:, k]\n",
    "                \n",
    "    return risk_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set prediction time window (t) and evaluation time (delta t) for C-index and Brier-Score)\n",
    "pred_time = list(range(16,33,1)) # prediction time (in months)\n",
    "eval_time = list(range(1))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/data_longi_expanded_var_for_dynamic_deephit_and_ts_extraction_2.csv')\n",
    "# df = pd.read_csv('./data/data_longi_expanded_var_for_dynamic_deephit.csv')\n",
    "\n",
    "\n",
    "trainingid_all = pd.read_csv('./data/all_training_set_ID_2.csv')\n",
    "validationid_all = pd.read_csv('./data/all_validation_set_ID_2.csv')\n",
    "testingid_all = pd.read_csv('./data/all_testing_set_ID_2.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>event</th>\n",
       "      <th>exam_year</th>\n",
       "      <th>time</th>\n",
       "      <th>AGE_Y0</th>\n",
       "      <th>MALE</th>\n",
       "      <th>RACEBLACK</th>\n",
       "      <th>ARMCI</th>\n",
       "      <th>ASMA</th>\n",
       "      <th>BEER</th>\n",
       "      <th>...</th>\n",
       "      <th>PULSE</th>\n",
       "      <th>SMKNW</th>\n",
       "      <th>WGT</th>\n",
       "      <th>WINE</th>\n",
       "      <th>WST</th>\n",
       "      <th>HBM</th>\n",
       "      <th>DBP</th>\n",
       "      <th>SBP</th>\n",
       "      <th>CHNOW</th>\n",
       "      <th>PATCK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100033323702</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32.134155</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>161.0</td>\n",
       "      <td>0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100033323702</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>32.134155</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>159.5</td>\n",
       "      <td>0</td>\n",
       "      <td>80.5</td>\n",
       "      <td>0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100033323702</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>32.134155</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>0</td>\n",
       "      <td>78.5</td>\n",
       "      <td>0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100033323702</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>32.134155</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>32.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>174.5</td>\n",
       "      <td>0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100033323702</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>32.134155</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>0</td>\n",
       "      <td>82.5</td>\n",
       "      <td>0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19983</th>\n",
       "      <td>416817227898</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>31.770021</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>199.5</td>\n",
       "      <td>0</td>\n",
       "      <td>95.5</td>\n",
       "      <td>0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19984</th>\n",
       "      <td>416817227898</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>31.770021</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>204.0</td>\n",
       "      <td>0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19985</th>\n",
       "      <td>416817227898</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>31.770021</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>196.0</td>\n",
       "      <td>0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19986</th>\n",
       "      <td>416817227898</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>31.770021</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.5</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19987</th>\n",
       "      <td>416817227898</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>31.770021</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>37.5</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>210.5</td>\n",
       "      <td>0</td>\n",
       "      <td>102.5</td>\n",
       "      <td>0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19988 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID  event  exam_year       time  AGE_Y0  MALE  RACEBLACK  \\\n",
       "0      100033323702      0          0  32.134155      22     1          0   \n",
       "1      100033323702      0          2  32.134155      22     1          0   \n",
       "2      100033323702      0          5  32.134155      22     1          0   \n",
       "3      100033323702      0          7  32.134155      22     1          0   \n",
       "4      100033323702      0         15  32.134155      22     1          0   \n",
       "...             ...    ...        ...        ...     ...   ...        ...   \n",
       "19983  416817227898      0          2  31.770021      22     1          1   \n",
       "19984  416817227898      0          5  31.770021      22     1          1   \n",
       "19985  416817227898      0          7  31.770021      22     1          1   \n",
       "19986  416817227898      0         10  31.770021      22     1          1   \n",
       "19987  416817227898      0         15  31.770021      22     1          1   \n",
       "\n",
       "       ARMCI  ASMA  BEER  ...  PULSE  SMKNW    WGT  WINE    WST  HBM    DBP  \\\n",
       "0        0.0     0     8  ...     34      1  161.0     0   75.0    0   74.0   \n",
       "1       30.0     1     2  ...     37      1  159.5     0   80.5    0   69.0   \n",
       "2       30.0     0     2  ...     30      0  169.0     0   78.5    0   73.0   \n",
       "3       32.5     0     1  ...     38      0  174.5     0   81.0    0   72.0   \n",
       "4       35.0     0     0  ...     38      0  178.0     0   82.5    0   71.0   \n",
       "...      ...   ...   ...  ...    ...    ...    ...   ...    ...  ...    ...   \n",
       "19983   38.0     0     8  ...     27      1  199.5     0   95.5    0   63.0   \n",
       "19984   36.0     0    24  ...     35      1  204.0     0  104.0    0  101.0   \n",
       "19985   36.0     0    28  ...     33      1  196.0     0   94.0    0   77.0   \n",
       "19986   35.5     0    12  ...     30      0  192.0     0   92.0    0   66.0   \n",
       "19987   37.5     0    16  ...     28      1  210.5     0  102.5    0   70.0   \n",
       "\n",
       "         SBP  CHNOW  PATCK  \n",
       "0      117.0      0      0  \n",
       "1      116.0      0      1  \n",
       "2      105.0      0      0  \n",
       "3      115.0      0      0  \n",
       "4      107.0      0      0  \n",
       "...      ...    ...    ...  \n",
       "19983  102.0      0      0  \n",
       "19984  133.0      0      0  \n",
       "19985  133.0      0      0  \n",
       "19986  112.0      0      0  \n",
       "19987  106.0      0      0  \n",
       "\n",
       "[19988 rows x 39 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_list = ['MALE', 'RACEBLACK', 'ASMA', 'CANCR', 'DIAB'\n",
    "                                 ,  'GALL', 'KIDNY', 'LIVER', 'MENTL', 'SMKNW', 'HBM', 'CHNOW', 'PATCK']\n",
    "cont_list = ['AGE_Y0', 'ARMCI', 'BEER', 'BMI', 'CGTDY', 'CHOL'\n",
    "       , 'ED', 'HDL', 'LDL', 'LIFE', 'LIQR'\n",
    "       , 'NPREG', 'NTRIG', 'PSTYR', 'PULSE', 'WGT'\n",
    "       , 'WINE', 'WST', 'DBP', 'SBP', 'GLU', 'DFPAY']\n",
    "len(bin_list)+len(cont_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_mode                   = 'PBC2' \n",
    "data_mode                   = 'CARDIA_ASCVD' \n",
    "seed                        = 1234\n",
    "\n",
    "##### IMPORT DATASET\n",
    "'''\n",
    "    num_Category            = max event/censoring time * 1.2\n",
    "    num_Event               = number of evetns i.e. len(np.unique(label))-1\n",
    "    max_length              = maximum number of measurements\n",
    "    x_dim                   = data dimension including delta (1 + num_features)\n",
    "    x_dim_cont              = dim of continuous features\n",
    "    x_dim_bin               = dim of binary features\n",
    "    mask1, mask2, mask3     = used for cause-specific network (FCNet structure)\n",
    "'''\n",
    "\n",
    "# (x_dim, x_dim_cont, x_dim_bin), (data, time, label), (mask1, mask2, mask3), (data_mi) = impt.import_dataset(norm_mode = 'standard')\n",
    "\n",
    "\n",
    "\n",
    "(x_dim, x_dim_cont, x_dim_bin), (data, time, label), (mask1, mask2, mask3), (data_mi) = impt.import_dataset(df_ = df\n",
    "                  , bin_list = bin_list\n",
    "                  , cont_list = cont_list\n",
    "                   , norm_mode = 'standard')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "_, num_Event, num_Category  = np.shape(mask1)  # dim of mask3: [subj, Num_Event, Num_Category]\n",
    "max_length                  = np.shape(data)[1]\n",
    "\n",
    "\n",
    "file_path = '{}'.format(data_mode)\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    os.makedirs(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3539, 6, 36)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "burn_in_mode                = 'ON' #{'ON', 'OFF'}\n",
    "boost_mode                  = 'ON' #{'ON', 'OFF'}\n",
    "\n",
    "##### HYPER-PARAMETERS\n",
    "new_parser = {'mb_size': 64, # 2 # 4 #64\n",
    "\n",
    "             'iteration_burn_in': 3000,\n",
    "             'iteration': 25000,\n",
    "\n",
    "             'keep_prob': 0.6, # 0.6\n",
    "             'lr_train': 1e-4,\n",
    "\n",
    "             'h_dim_RNN': 100,\n",
    "             'h_dim_FC' : 100,\n",
    "             'num_layers_RNN':2, #8 #6 #4 #2\n",
    "             'num_layers_ATT':2,\n",
    "             'num_layers_CS' :2,\n",
    "\n",
    "             'RNN_type':'LSTM', #{'LSTM', 'GRU'}\n",
    "\n",
    "             'FC_active_fn' : tf.nn.relu,\n",
    "             'RNN_active_fn': tf.nn.tanh,\n",
    "\n",
    "            'reg_W'         : 1e-5,\n",
    "            'reg_W_out'     : 0.,\n",
    "\n",
    "             'alpha' :1.0,\n",
    "             'beta'  :0.1,\n",
    "             'gamma' :1.0\n",
    "}\n",
    "\n",
    "\n",
    "# INPUT DIMENSIONS\n",
    "input_dims                  = { 'x_dim'         : x_dim,\n",
    "                                'x_dim_cont'    : x_dim_cont,\n",
    "                                'x_dim_bin'     : x_dim_bin,\n",
    "                                'num_Event'     : num_Event,\n",
    "                                'num_Category'  : num_Category,\n",
    "                                'max_length'    : max_length }\n",
    "\n",
    "# NETWORK HYPER-PARMETERS\n",
    "network_settings            = { 'h_dim_RNN'         : new_parser['h_dim_RNN'],\n",
    "                                'h_dim_FC'          : new_parser['h_dim_FC'],\n",
    "                                'num_layers_RNN'    : new_parser['num_layers_RNN'],\n",
    "                                'num_layers_ATT'    : new_parser['num_layers_ATT'],\n",
    "                                'num_layers_CS'     : new_parser['num_layers_CS'],\n",
    "                                'RNN_type'          : new_parser['RNN_type'],\n",
    "                                'FC_active_fn'      : new_parser['FC_active_fn'],\n",
    "                                'RNN_active_fn'     : new_parser['RNN_active_fn'],\n",
    "                               # 'initial_W'         : tf.contrib.layers.xavier_initializer(),\n",
    "                               \n",
    "                                'initial_W'         : tf.keras.initializers.glorot_normal(),\n",
    "\n",
    "                               \n",
    "                                'reg_W'             : new_parser['reg_W'],\n",
    "                                'reg_W_out'         : new_parser['reg_W_out']\n",
    "                                 }\n",
    "\n",
    "\n",
    "mb_size           = new_parser['mb_size']\n",
    "iteration         = new_parser['iteration']\n",
    "iteration_burn_in = new_parser['iteration_burn_in']\n",
    "\n",
    "keep_prob         = new_parser['keep_prob']\n",
    "lr_train          = new_parser['lr_train']\n",
    "\n",
    "alpha             = new_parser['alpha']\n",
    "beta              = new_parser['beta']\n",
    "gamma             = new_parser['gamma']\n",
    "\n",
    "# SAVE HYPERPARAMETERS\n",
    "log_name = file_path + '/hyperparameters_log.txt'\n",
    "save_logging(new_parser, log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 1...\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "BURN-IN TRAINING ...\n",
      "itr: 1000 | loss: 2.3523\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-0ba1eedb94e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mMISSING\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_mi_mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_curr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_burn_in\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMISSING\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m1000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mU:\\Hieu\\CARDIA_longi_project\\Dynamic-DeepHit-master\\class_DeepLongitudinal.py\u001b[0m in \u001b[0;36mtrain_burn_in\u001b[1;34m(self, DATA, MISSING, keep_prob, lr_train)\u001b[0m\n\u001b[0;32m    314\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[0mx_mi_mb\u001b[0m\u001b[1;33m)\u001b[0m                 \u001b[1;33m=\u001b[0m \u001b[0mMISSING\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m         return self.sess.run([self.solver_burn_in, self.LOSS_3], \n\u001b[0m\u001b[0;32m    317\u001b[0m                              feed_dict={self.x:x_mb, self.x_mi: x_mi_mb, self.k:k_mb, self.t:t_mb, \n\u001b[0;32m    318\u001b[0m                                         self.mb_size: np.shape(x_mb)[0], self.keep_prob:keep_prob, self.lr_rate:lr_train})\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 967\u001b[1;33m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[0;32m    968\u001b[0m                          run_metadata_ptr)\n\u001b[0;32m    969\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1188\u001b[0m     \u001b[1;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1189\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1190\u001b[1;33m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[0;32m   1191\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[0;32m   1192\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1368\u001b[1;33m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[0;32m   1369\u001b[0m                            run_metadata)\n\u001b[0;32m   1370\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1373\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1374\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1375\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1376\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1377\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1359\u001b[1;33m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0m\u001b[0;32m   1360\u001b[0m                                       target_list, run_metadata)\n\u001b[0;32m   1361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1449\u001b[0m   def _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list,\n\u001b[0;32m   1450\u001b[0m                           run_metadata):\n\u001b[1;32m-> 1451\u001b[1;33m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0m\u001b[0;32m   1452\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m                                             run_metadata)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Tuning: training with number of iteration 25000 -> 50000\n",
    "\n",
    "fold = 1\n",
    "print('FOLD '+str(fold) + '...')\n",
    "\n",
    "##### get training, testing, and validation data:\n",
    "df_train = df.loc[df['ID'].isin(trainingid_all.iloc[:,fold])]\n",
    "df_val = df.loc[df['ID'].isin(validationid_all.iloc[:,fold])]\n",
    "df_test = df.loc[df['ID'].isin(testingid_all.iloc[:,fold])]\n",
    "\n",
    "# ### TRAINING-TESTING SPLIT in the format suitable for this network\n",
    "\n",
    "(x_dim, x_dim_cont, x_dim_bin), (te_data, te_time, te_label), (te_mask1, te_mask2, te_mask3), (te_data_mi) = impt.import_dataset(df_ = df_test)\n",
    "(x_dim, x_dim_cont, x_dim_bin), (va_data, va_time, va_label), (va_mask1, va_mask2, va_mask3), (va_data_mi) = impt.import_dataset(df_ = df_val)\n",
    "(x_dim, x_dim_cont, x_dim_bin), (tr_data, tr_time, tr_label), (tr_mask1, tr_mask2, tr_mask3), (tr_data_mi) = impt.import_dataset(df_ = df_train)\n",
    "\n",
    "if boost_mode == 'ON':\n",
    "    tr_data, tr_data_mi, tr_time, tr_label, tr_mask1, tr_mask2, tr_mask3 = f_get_boosted_trainset(tr_data, tr_data_mi, tr_time, tr_label, tr_mask1, tr_mask2, tr_mask3)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### CREATE AND TRAIN NETWORK:\n",
    "# tf.reset_default_graph()\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# config = tf.ConfigProto()\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "\n",
    "model = Model_Longitudinal_Attention(sess, \"Dyanmic-DeepHit\", input_dims, network_settings)\n",
    "# saver = tf.train.Saver()\n",
    "saver = tf.compat.v1.train.Saver()\n",
    "\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "### TRAINING - BURN-IN\n",
    "if burn_in_mode == 'ON':\n",
    "    print( \"BURN-IN TRAINING ...\")\n",
    "    for itr in range(iteration_burn_in):\n",
    "        x_mb, x_mi_mb, k_mb, t_mb, m1_mb, m2_mb, m3_mb = f_get_minibatch(mb_size, tr_data, tr_data_mi, tr_label, tr_time, tr_mask1, tr_mask2, tr_mask3)\n",
    "        DATA = (x_mb, k_mb, t_mb)\n",
    "        MISSING = (x_mi_mb)\n",
    "\n",
    "        _, loss_curr = model.train_burn_in(DATA, MISSING, keep_prob, lr_train)\n",
    "\n",
    "        if (itr+1)%1000 == 0:\n",
    "            print('itr: {:04d} | loss: {:.4f}'.format(itr+1, loss_curr))\n",
    "\n",
    "\n",
    "### TRAINING - MAIN\n",
    "print( \"MAIN TRAINING ...\")\n",
    "min_valid = 0.5\n",
    "\n",
    "for itr in range(iteration):\n",
    "    x_mb, x_mi_mb, k_mb, t_mb, m1_mb, m2_mb, m3_mb = f_get_minibatch(mb_size, tr_data, tr_data_mi, tr_label, tr_time, tr_mask1, tr_mask2, tr_mask3)\n",
    "    DATA = (x_mb, k_mb, t_mb)\n",
    "    MASK = (m1_mb, m2_mb, m3_mb)\n",
    "    MISSING = (x_mi_mb)\n",
    "    PARAMETERS = (alpha, beta, gamma)\n",
    "\n",
    "    _, loss_curr = model.train(DATA, MASK, MISSING, PARAMETERS, keep_prob, lr_train)\n",
    "\n",
    "    if (itr+1)%1000 == 0:\n",
    "        print('itr: {:04d} | loss: {:.4f}'.format(itr+1, loss_curr))\n",
    "\n",
    "    ### VALIDATION  (based on average C-index of our interest)\n",
    "    if (itr+1)%1000 == 0:        \n",
    "        risk_all = f_get_risk_predictions(sess, model, va_data, va_data_mi, pred_time, eval_time)\n",
    "\n",
    "        for p, p_time in enumerate(pred_time):\n",
    "            pred_horizon = int(p_time)\n",
    "            val_result1 = np.zeros([num_Event, len(eval_time)])\n",
    "\n",
    "            for t, t_time in enumerate(eval_time):                \n",
    "                eval_horizon = int(t_time) + pred_horizon\n",
    "                for k in range(num_Event):\n",
    "                    val_result1[k, t] = c_index(risk_all[k][:, p, t], va_time, (va_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
    "\n",
    "            if p == 0:\n",
    "                val_final1 = val_result1\n",
    "            else:\n",
    "                val_final1 = np.append(val_final1, val_result1, axis=0)\n",
    "\n",
    "        tmp_valid = np.mean(val_final1)\n",
    "\n",
    "        if tmp_valid >  min_valid:\n",
    "            min_valid = tmp_valid\n",
    "            saver.save(sess, file_path + '/model')\n",
    "            print( 'updated.... average c-index = ' + str('%.4f' %(tmp_valid)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ### PREDICTION ON TEST SET               \n",
    "# #saver.restore(sess, file_path + '/model')\n",
    "\n",
    "# risk_all = f_get_risk_predictions(sess, model, te_data, te_data_mi, pred_time, eval_time)\n",
    "\n",
    "\n",
    "# for p, p_time in enumerate(pred_time):\n",
    "#     pred_horizon = int(p_time)\n",
    "#     result1, result2 = np.zeros([num_Event, len(eval_time)]), np.zeros([num_Event, len(eval_time)])\n",
    "\n",
    "#     for t, t_time in enumerate(eval_time):                \n",
    "#         eval_horizon = int(t_time) + pred_horizon\n",
    "#         for k in range(num_Event):\n",
    "#             result1[k, t] = c_index(risk_all[k][:, p, t], te_time, (te_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
    "#             result2[k, t] = brier_score(risk_all[k][:, p, t], te_time, (te_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
    "\n",
    "#     if p == 0:\n",
    "#         final1, final2 = result1, result2\n",
    "#     else:\n",
    "#         final1, final2 = np.append(final1, result1, axis=0), np.append(final2, result2, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ### PRINT PERFORMANCE RESULTS\n",
    "# row_header = []\n",
    "# for p_time in pred_time:\n",
    "#     for t in range(num_Event):\n",
    "#         row_header.append('pred_time {}: event_{}'.format(p_time,k+1))\n",
    "\n",
    "# col_header = []\n",
    "# for t_time in eval_time:\n",
    "#     col_header.append('eval_time {}'.format(t_time))\n",
    "\n",
    "\n",
    "# # c-index result\n",
    "# df1 = pd.DataFrame(final1, index = row_header, columns=col_header)\n",
    "\n",
    "# # brier-score result\n",
    "# df2 = pd.DataFrame(final2, index = row_header, columns=col_header)\n",
    "\n",
    "# print('========================================================')\n",
    "# print('--------------------------------------------------------')\n",
    "# print('- C-INDEX: ')\n",
    "# print(df1)\n",
    "# print('--------------------------------------------------------')\n",
    "# print('- BRIER-SCORE: ')\n",
    "# print(df2)\n",
    "# print('========================================================')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ### SAVE C-INDEX, BRIER SCORE, and PREDICTED PROB RISK ON TEST SET\n",
    "# actual_fold = fold+1\n",
    "# work_dir = 'U:/Hieu/CARDIA_longi_project'\n",
    "# savedir = os.path.join(work_dir,'rdata_files/dynamic_deephit_expanded_var_y15_2_fold_'+str(actual_fold)+'/')\n",
    "# try: \n",
    "#     os.makedirs(savedir)\n",
    "# except OSError:\n",
    "#     if not os.path.isdir(savedir):\n",
    "#         raise\n",
    "\n",
    "\n",
    "\n",
    "# c_over_time = df1.iloc[:,0]\n",
    "# # c_over_time.to_csv(savedir+'/c_index.csv', index = None, header = True)\n",
    "\n",
    "# brier_over_time = df2.iloc[:,0]\n",
    "# # brier_over_time.to_csv(savedir+'/brier_score.csv', index = None, header = True)\n",
    "\n",
    "\n",
    "\n",
    "# prob_risk_test_df = pd.DataFrame(risk_all[0][:,:,0])\n",
    "# prob_risk_test_df.columns = pred_time\n",
    "# prob_risk_test_df.insert(loc=0, column='ID', value=np.unique(df_test['ID']))\n",
    "# # prob_risk_test_df.to_csv(savedir+'/prob_risk_test.csv', index = None, header = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert from long format to \n",
    "# varying_var =['CHOL','DIAB','HBM','HDL','SBP','SMKNW']\n",
    "# varying_var = ['ASMA', 'CANCR', 'DIAB'\n",
    "#                                  ,  'GALL', 'KIDNY', 'LIVER', 'MENTL', 'SMKNW', 'HBM', 'CHNOW', 'PATCK'\n",
    "#                 ,'ARMCI', 'BEER', 'BMI', 'CGTDY', 'CHOL'\n",
    "#        , 'ED', 'HDL', 'LDL', 'LIFE', 'LIQR'\n",
    "#        , 'NPREG', 'NTRIG', 'PSTYR', 'PULSE', 'WGT'\n",
    "#        , 'WINE', 'WST', 'DBP', 'SBP', 'GLU', 'DFPAY']\n",
    "# n_exam = 6\n",
    "# n_vars = 6\n",
    "# n_subjects = 3539\n",
    "\n",
    "def convert_long_format_to_3d_matrix(data_ts_, n_exam_, n_varying_vars_, n_subjects_, varying_var_):\n",
    "\n",
    "    desired_X = np.array([[[0 for k in range(n_exam_)] for j in range(n_varying_vars_)] for i in range(n_subjects_)])\n",
    "    for i in range(0,n_varying_vars_):\n",
    "        var_wide_df = data_ts_.pivot(index='ID', columns='exam_year', values=varying_var_[i])\n",
    "        var_wide_df_locf = var_wide_df.T.fillna(method='ffill').T\n",
    "        var_wide_locf_matrix = var_wide_df_locf.values\n",
    "        desired_X[:,i,:] = var_wide_locf_matrix\n",
    "    return(desired_X)\n",
    "\n",
    "def convert_3d_to_long_format(X, n_subjects_, varying_var_,exam_year_list_, unique_ids_arr_):\n",
    "    for subject in range(0, n_subjects_): \n",
    "        if (subject == 0):\n",
    "            all_subject_long_format = X[subject,:,:].T\n",
    "        else:\n",
    "            subject_long_format = desired_X[subject,:,:].T\n",
    "            all_subject_long_format = np.concatenate((all_subject_long_format, subject_long_format))\n",
    "    all_subject_long_format_df = pd.DataFrame(all_subject_long_format)\n",
    "    all_subject_long_format_df.columns = varying_var_\n",
    "    all_subject_long_format_df.insert(loc = 0, column = 'exam_year',\n",
    "              value = exam_year_list_*n_subjects_)\n",
    "    all_subject_long_format_df.insert(loc = 0, column = 'ID',\n",
    "              value = [val for val in unique_ids_arr_ for _ in range(len(exam_year_list_))])\n",
    "    return(all_subject_long_format_df)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "\n",
    "class dynamic_deephit_TIME():\n",
    "    \"\"\"Class implementing model API required by anamod\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = kwargs\n",
    "        self.dd_time = DYNAMIC_DEEPHIT(**kwargs)\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \n",
    "        X_dynamic_deephit_format = dynamic_deephit_reformat(X)\n",
    "        self.dd_time.fit(X_dynamic_deephit_format, y)\n",
    "\n",
    "\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Perform prediction on input X (comprising one or more instances)\"\"\"\n",
    "        # return self.ground_truth_model.predict(X, noise=self.noise_multiplier)\n",
    "        X_long_format_df = dynamic_deephit_reformat(X)\n",
    "        (x_dim, x_dim_cont, x_dim_bin), (te_data, te_time, te_label), (te_mask1, te_mask2, te_mask3), (te_data_mi) = impt.import_dataset(df_ = X_long_format_df)\n",
    "\n",
    "        risk_all = f_get_risk_predictions(sess, self.dd_time, te_data, te_data_mi, pred_time, eval_time)\n",
    "        prob_risk_test_matrix = risk_all[0][:,:,0]\n",
    "        preds = prob_risk_test_matrix[:, -1]\n",
    "        return(preds)\n",
    "    \n",
    "    def score(self, X, y_label):\n",
    "        X_rsf_format = rsf_ts_reformat(X)\n",
    "        c_index = self.rsf_ts.score(X_rsf_format, y_label)\n",
    "        return(c_index)\n",
    "    \n",
    "# from sksurv.ensemble import RandomSurvivalForest\n",
    "\n",
    "# class RSF_TS():\n",
    "#     \"\"\"Class implementing model API required by anamod\"\"\"\n",
    "#     def __init__(self, **kwargs):\n",
    "#         self.kwargs = kwargs\n",
    "#         self.rsf_ts = RandomSurvivalForest(**kwargs)\n",
    "        \n",
    "#     def fit(self, X, y=None):\n",
    "        \n",
    "#         X_rsf_format = rsf_ts_reformat(X)\n",
    "#         self.rsf_ts.fit(X_rsf_format, y)\n",
    "\n",
    "#         # y_rsf_format = sksurv.util.Surv.from_dataframe('event', 'time', data_for_training_tsfeatures_models)\n",
    "#         # self.rsf_ts.fit(X_rsf_format, y_rsf_format)\n",
    "\n",
    "        \n",
    "#     def predict(self, X):\n",
    "#         \"\"\"Perform prediction on input X (comprising one or more instances)\"\"\"\n",
    "#         # return self.ground_truth_model.predict(X, noise=self.noise_multiplier)\n",
    "#         X_rsf_format = rsf_ts_reformat(X)\n",
    "#         # preds = self.rsf_ts.predict(X_rsf_format)\n",
    "#         surv = self.rsf_ts.predict_survival_function(X_rsf_format, return_array = True)\n",
    "#         preds = surv[:, -1]\n",
    "#         return(preds)\n",
    "    \n",
    "#     def score(self, X, y_label):\n",
    "#         X_rsf_format = rsf_ts_reformat(X)\n",
    "#         c_index = self.rsf_ts.score(X_rsf_format, y_label)\n",
    "#         return(c_index)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
